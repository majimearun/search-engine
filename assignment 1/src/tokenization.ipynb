{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import glob\n",
    "import pickle\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pickle_into_pages(files):\n",
    "    \"\"\"\n",
    "    Read the pickle file and return the text as a list of pages.\n",
    "    \"\"\"\n",
    "    main_df = pd.DataFrame(columns=[\"document_name\", \"page_number\", \"text\"])\n",
    "    for file in files:\n",
    "        fh = open(file, \"rb\")\n",
    "        doc_info = pickle.load(fh)\n",
    "        text = [page[0] for page in doc_info[\"text\"]]\n",
    "        page_number = [page[1] for page in doc_info[\"text\"]]\n",
    "        df = pd.DataFrame(columns=[\"document_name\", \"page_number\", \"text\"])\n",
    "        df[\"document_name\"] = [file.split(\"/\")[-1][:-4]] * len(text)\n",
    "        df[\"page_number\"] = page_number\n",
    "        df[\"text\"] = text\n",
    "        main_df = pd.concat([main_df, df])\n",
    "    return main_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_pkls = glob.glob(\n",
    "    \"/home/majime/programming/github/information-retrieval-assignments/assignment 1/pkls/Auto/*.pkl\"\n",
    ")\n",
    "property_pkls = glob.glob(\n",
    "    \"/home/majime/programming/github/information-retrieval-assignments/assignment 1/pkls/Property/*.pkl\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_df = read_pickle_into_pages(auto_pkls)\n",
    "property_df = read_pickle_into_pages(property_pkls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seperate_df_into_paragraphs(df):\n",
    "    \"\"\"\n",
    "    Seperate the dataframe into paragraphs.\n",
    "    \"\"\"\n",
    "    main_df = pd.DataFrame(columns=[\"document_name\", \"page_number\", \"paragraph_number\", \"text\"])\n",
    "    for index, row in df.iterrows():\n",
    "        text = row[\"text\"].split(\"\\n \\n\")\n",
    "        # remove empty strings\n",
    "        text = [paragraph for paragraph in text if paragraph != \"\"]\n",
    "        # if only spaces, remove\n",
    "        text = [paragraph for paragraph in text if paragraph != \" \"]\n",
    "        for paragraph_number, paragraph in enumerate(text):\n",
    "            df = pd.DataFrame(columns=[\"document_name\", \"page_number\", \"paragraph_number\", \"text\"])\n",
    "            df[\"document_name\"] = [row[\"document_name\"]]\n",
    "            df[\"page_number\"] = [row[\"page_number\"]]\n",
    "            df[\"paragraph_number\"] = [paragraph_number]\n",
    "            df[\"text\"] = [paragraph]\n",
    "            main_df = pd.concat([main_df, df])\n",
    "    return main_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_final = seperate_df_into_paragraphs(auto_df)\n",
    "auto_final.reset_index(inplace=True, drop=True)\n",
    "\n",
    "property_final = seperate_df_into_paragraphs(property_df)\n",
    "property_final.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(df, nlp, allow_digits = False, allow_punct = False, allow_stopwords = False, allow_numbers = False):\n",
    "    df[\"tokenized\"] = df[\"text\"].apply(lambda x: \" \".join([token.text for token in nlp(x) if (token.is_alpha or allow_digits) and (not token.is_punct or allow_punct) and (not token.is_stop or allow_stopwords) and (not token.like_num or allow_numbers)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize(auto_final, nlp)\n",
    "tokenize(property_final, nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_final.to_csv(\"/home/majime/programming/github/information-retrieval-assignments/assignment 1/tokenized/auto.csv\", index=False)\n",
    "property_final.to_csv(\"/home/majime/programming/github/information-retrieval-assignments/assignment 1/tokenized/property.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "info_retrieval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "658a82028ab00f880bc6bcdff014f37899cf9fd6fbfdda117e2e46094372bdf4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
